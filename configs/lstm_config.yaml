# LSTM Baseline Configuration

model:
  name: "LSTM"
  input_size: 1  # Number of features per timestep
  hidden_size: 128
  num_layers: 3
  dropout: 0.2
  bidirectional: false
  
data:
  data_dir: "src/models/data/processed"
  sequence_length: 5  # 5 input years + 1 output = 6 years minimum needed
  forecast_horizon: 1
  train_years: [2010, 2017]  # 8 years for training (creates ~3 samples per series)
  val_years: [2014, 2020]    # 7 years for validation (includes some train overlap for context)
  test_years: [2017, 2023]   # 7 years for testing (includes some val overlap for context)
  batch_size: 64
  num_workers: 0  # Set to 0 for compatibility

training:
  max_epochs: 150
  learning_rate: 0.001
  weight_decay: 0.0001
  optimizer: "adam"
  scheduler: "step"
  step_size: 30
  gamma: 0.5
  gradient_clip_val: 1.0
  early_stopping_patience: 15

logging:
  experiment_name: "lstm_baseline"
  log_dir: "results/logs"
  checkpoint_dir: "results/checkpoints"
  
reproducibility:
  seed: 42

